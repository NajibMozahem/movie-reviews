{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to train a neural network on the movie reviews data set. The prupose is for the model to predict whether a review is positive or negative.\n",
    "\n",
    "First we download the data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the datasets dictionary, we will see that it contains the train and test data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'unsupervised'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops._OptionsDataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = datasets[\"train\"]\n",
    "test_set = datasets[\"test\"]\n",
    "type(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data sets are stored as tensorflows. Let us take a look at a record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Well let me go say this because i love history and I know that movie is most important piece in our history and it was beautifully executed movie and Julia Stiles became my #1 favorite actress after seeing her in \"The \\'60s\" and i own this movie in my video box with many movies and i suggest you to look for her new movies in the future and try to enjoy history!!!!'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Just because someone is under the age of 10 does not mean they are stupid. If your child likes this film you\\'d better have him/her tested. I am continually amazed at how so many people can be involved in something that turns out so bad. This \"film\" is a showcase for digital wizardry AND NOTHING ELSE. The writing is horrid. I can\\'t remember when I\\'ve heard such bad dialogue. The songs are beyond wretched. The acting is sub-par but then the actors were not given much. Who decided to employ Joey Fatone? He cannot sing and he is ugly as sin.<br /><br />The worst thing is the obviousness of it all. It is as if the writers went out of their way to make it all as stupid as possible. Great children\\'s movies are wicked, smart and full of wit - films like Shrek and Toy Story in recent years, Willie Wonka and The Witches to mention two of the past. But in the continual dumbing-down of American more are flocking to dreck like Finding Nemo (yes, that\\'s right), the recent Charlie & The Chocolate Factory and eye-crossing trash like Red Riding Hood.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "for element in train_set.take(2):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that there are two tensors per record, one is the review and the other is a numbe (0 or 1) that indicates whether the review is positive or negative.\n",
    "\n",
    "We will need to vectorize the text before we train the model. Here we have two options. We can either do the vectorization ourself, or we can use a pre-trained embedding. We will do both. For the first model, we will do the text vectorization and embedding ourself. To do that, we will need to use a vectorizer. We will use a vocabulary of size 10,000, and we will use a max length of 300 for the reviews. Reviews shroter than this will be padded. Longer reviews will be cut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "max_length = 300\n",
    "\n",
    "int_vectorize_layer = keras.layers.TextVectorization(max_tokens=vocabulary_size, output_mode='int', output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to use the vectorizer to convert the text to indices. However, before we do that, it would be nice to clean up the text. If you look at the text of the reviews you will notice that there are the characters some tags that sstart and end with <>, specifically with the word br in them. These tags do not help much in sentiment analysis. We can also remove digits if they exist, since we just need text. We will do this cleaning for both the training set and the validation set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.map(lambda x_text, x_label: (tf.strings.regex_replace(x_text, \"<br />\", \" \"), x_label))\n",
    "train_set = train_set.map(lambda x_text, x_label: (tf.strings.regex_replace(x_text, \"[^a-zA-Z']\", \" \"), x_label))\n",
    "\n",
    "test_set = test_set.map(lambda x_text, x_label: (tf.strings.regex_replace(x_text, \"<br />\", \" \"), x_label))\n",
    "test_set = test_set.map(lambda x_text, x_label: (tf.strings.regex_replace(x_text, \"[^a-zA-Z']\", \" \"), x_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the modified text to prepare the vectorizer. To do that, we will need to call the adapt function. This function will adapt the vectorizer to the text. First, we need an object that contains only the text, since the train set contains tensors that contain the text and tensors that contain the outcome variable. So we create this object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This is the most depressing film I have ever seen  I first saw it as a child and even thinking about it now really upsets me  I know it was set in a time when life was hard and I know these people were poor and the crops were vital  Yes  I get all that  What I find hard to take is I can't remember one single light moment in the entire film  Maybe it was true to life  I don't know  I'm quite sure the acting was top notch and the direction and quality of filming etc etc was wonderful and I know that every film can't have a happy ending but as a family film it is dire in my opinion   I wouldn't recommend it to anyone who wants to be entertained by a film  I can't stress enough how this film affected me as a child  I was talking about it recently and all the sad memories came flooding back  I think it would have all but the heartless reaching for the Prozac \", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "train_text = train_set.map(lambda text, labels: text)\n",
    "for text in train_text.take(1):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a data set that contains only the text. W ecan use this data set to setup the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the words that are indexed using the vectorizer you use the get_vocabulary function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'and',\n",
       " 'a',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'in',\n",
       " 'it',\n",
       " 'i',\n",
       " 'this',\n",
       " 'that',\n",
       " 'was',\n",
       " 'as',\n",
       " 'for',\n",
       " 'with',\n",
       " 'movie',\n",
       " 'but',\n",
       " 'film',\n",
       " 'on',\n",
       " 'not',\n",
       " 'you',\n",
       " 'are',\n",
       " 'his',\n",
       " 'have',\n",
       " 'he',\n",
       " 'be',\n",
       " 'one',\n",
       " 'its',\n",
       " 'all',\n",
       " 'at',\n",
       " 'by',\n",
       " 'an',\n",
       " 'they',\n",
       " 'who',\n",
       " 'so',\n",
       " 'from',\n",
       " 'like',\n",
       " 'her',\n",
       " 'or',\n",
       " 'just',\n",
       " 'about',\n",
       " 'out',\n",
       " 'if',\n",
       " 'has',\n",
       " 'there',\n",
       " 'some',\n",
       " 'what',\n",
       " 'good',\n",
       " 'more',\n",
       " 'when',\n",
       " 'very',\n",
       " 'up',\n",
       " 'no',\n",
       " 'time',\n",
       " 'she',\n",
       " 'even',\n",
       " 'my',\n",
       " 'would',\n",
       " 'which',\n",
       " 'story',\n",
       " 'only',\n",
       " 'really',\n",
       " 'see',\n",
       " 'their',\n",
       " 'were',\n",
       " 'had',\n",
       " 'can',\n",
       " 'well',\n",
       " 'me',\n",
       " 'than',\n",
       " 'we',\n",
       " 'much',\n",
       " 'bad',\n",
       " 'been',\n",
       " 'get',\n",
       " 'will',\n",
       " 'do',\n",
       " 'also',\n",
       " 'people',\n",
       " 'into',\n",
       " 'other',\n",
       " 'first',\n",
       " 'great',\n",
       " 'because',\n",
       " 'how',\n",
       " 'him',\n",
       " 'most',\n",
       " 'dont',\n",
       " 'made',\n",
       " 'then',\n",
       " 'movies',\n",
       " 'way',\n",
       " 'make',\n",
       " 'them',\n",
       " 'films',\n",
       " 'too',\n",
       " 'could',\n",
       " 'any',\n",
       " 'after',\n",
       " 'characters',\n",
       " 'think',\n",
       " 'watch',\n",
       " 'two',\n",
       " 'character',\n",
       " 'seen',\n",
       " 'many',\n",
       " 'being',\n",
       " 'life',\n",
       " 'plot',\n",
       " 'acting',\n",
       " 'never',\n",
       " 'little',\n",
       " 'love',\n",
       " 'best',\n",
       " 'over',\n",
       " 'where',\n",
       " 'did',\n",
       " 'show',\n",
       " 'know',\n",
       " 's',\n",
       " 'off',\n",
       " 'ever',\n",
       " 'does',\n",
       " 'better',\n",
       " 'your',\n",
       " 'end',\n",
       " 'man',\n",
       " 'still',\n",
       " 'here',\n",
       " 'these',\n",
       " 'say',\n",
       " 'scene',\n",
       " 'while',\n",
       " 'why',\n",
       " 'scenes',\n",
       " 'go',\n",
       " 'such',\n",
       " 'something',\n",
       " 'back',\n",
       " 'through',\n",
       " 'should',\n",
       " 'im',\n",
       " 'real',\n",
       " 'those',\n",
       " 'watching',\n",
       " 'now',\n",
       " 'years',\n",
       " 'though',\n",
       " 'doesnt',\n",
       " 'actors',\n",
       " 'old',\n",
       " 'thing',\n",
       " 'work',\n",
       " 'didnt',\n",
       " 'before',\n",
       " 'another',\n",
       " 'new',\n",
       " 'funny',\n",
       " 'nothing',\n",
       " 'actually',\n",
       " 'makes',\n",
       " 'director',\n",
       " 'look',\n",
       " 'find',\n",
       " 'going',\n",
       " 'few',\n",
       " 'same',\n",
       " 'part',\n",
       " 'again',\n",
       " 'every',\n",
       " 'lot',\n",
       " 'cast',\n",
       " 'us',\n",
       " 'thats',\n",
       " 'quite',\n",
       " 'cant',\n",
       " 'down',\n",
       " 'things',\n",
       " 'world',\n",
       " 'want',\n",
       " 'pretty',\n",
       " 'young',\n",
       " 'seems',\n",
       " 'around',\n",
       " 'got',\n",
       " 'horror',\n",
       " 'however',\n",
       " 'fact',\n",
       " 'take',\n",
       " 'big',\n",
       " 'enough',\n",
       " 'long',\n",
       " 'thought',\n",
       " 'series',\n",
       " 'both',\n",
       " 'between',\n",
       " 'give',\n",
       " 'may',\n",
       " 'original',\n",
       " 'ive',\n",
       " 'action',\n",
       " 'own',\n",
       " 'right',\n",
       " 'without',\n",
       " 'times',\n",
       " 'always',\n",
       " 'comedy',\n",
       " 'gets',\n",
       " 'point',\n",
       " 'must',\n",
       " 'come',\n",
       " 'isnt',\n",
       " 'role',\n",
       " 'saw',\n",
       " 'almost',\n",
       " 'interesting',\n",
       " 'least',\n",
       " 'family',\n",
       " 'theres',\n",
       " 'done',\n",
       " 'whole',\n",
       " 'bit',\n",
       " 'music',\n",
       " 'script',\n",
       " 'far',\n",
       " 'guy',\n",
       " 'making',\n",
       " 'minutes',\n",
       " 'feel',\n",
       " 'anything',\n",
       " 'last',\n",
       " 'hes',\n",
       " 'since',\n",
       " 'performance',\n",
       " 'might',\n",
       " 'probably',\n",
       " 'am',\n",
       " 'kind',\n",
       " 'away',\n",
       " 'yet',\n",
       " 'girl',\n",
       " 'tv',\n",
       " 'rather',\n",
       " 'worst',\n",
       " 'day',\n",
       " 'fun',\n",
       " 'sure',\n",
       " 'hard',\n",
       " 'woman',\n",
       " 'played',\n",
       " 'each',\n",
       " 'found',\n",
       " 'anyone',\n",
       " 'having',\n",
       " 'although',\n",
       " 'especially',\n",
       " 'our',\n",
       " 'course',\n",
       " 'believe',\n",
       " 'screen',\n",
       " 'comes',\n",
       " 'looking',\n",
       " 'trying',\n",
       " 'set',\n",
       " 'goes',\n",
       " 'shows',\n",
       " 'looks',\n",
       " 'place',\n",
       " 'book',\n",
       " 'different',\n",
       " 'put',\n",
       " 'ending',\n",
       " 'money',\n",
       " 'maybe',\n",
       " 'wasnt',\n",
       " 'true',\n",
       " 'once',\n",
       " 'sense',\n",
       " 'reason',\n",
       " 'actor',\n",
       " 'everything',\n",
       " 'dvd',\n",
       " 'year',\n",
       " 'three',\n",
       " 'worth',\n",
       " 'job',\n",
       " 'main',\n",
       " 'someone',\n",
       " 'together',\n",
       " 'watched',\n",
       " 'play',\n",
       " 'plays',\n",
       " 'american',\n",
       " 'effects',\n",
       " 'later',\n",
       " 'said',\n",
       " 'takes',\n",
       " 'instead',\n",
       " 'beautiful',\n",
       " 'john',\n",
       " 'seem',\n",
       " 'house',\n",
       " 'himself',\n",
       " 'high',\n",
       " 'night',\n",
       " 'version',\n",
       " 'audience',\n",
       " 'during',\n",
       " 'everyone',\n",
       " 'left',\n",
       " 'special',\n",
       " 'seeing',\n",
       " 'half',\n",
       " 'star',\n",
       " 'excellent',\n",
       " 'wife',\n",
       " 'shot',\n",
       " 'war',\n",
       " 'idea',\n",
       " 'black',\n",
       " 'nice',\n",
       " 'less',\n",
       " 'mind',\n",
       " 'simply',\n",
       " 'read',\n",
       " 'second',\n",
       " 'father',\n",
       " 'youre',\n",
       " 'else',\n",
       " 'kids',\n",
       " 'death',\n",
       " 'fan',\n",
       " 'poor',\n",
       " 'help',\n",
       " 'completely',\n",
       " 'used',\n",
       " 'home',\n",
       " 'dead',\n",
       " 'line',\n",
       " 'either',\n",
       " 'short',\n",
       " 'men',\n",
       " 'friends',\n",
       " 'top',\n",
       " 'given',\n",
       " 'budget',\n",
       " 'try',\n",
       " 'classic',\n",
       " 'wrong',\n",
       " 'performances',\n",
       " 'boring',\n",
       " 'enjoy',\n",
       " 'need',\n",
       " 'hollywood',\n",
       " 'use',\n",
       " 'rest',\n",
       " 'low',\n",
       " 'production',\n",
       " 'until',\n",
       " 'full',\n",
       " 'along',\n",
       " 'camera',\n",
       " 'truly',\n",
       " 'women',\n",
       " 'video',\n",
       " 'awful',\n",
       " 'tell',\n",
       " 'next',\n",
       " 'stars',\n",
       " 'remember',\n",
       " 'stupid',\n",
       " 'couple',\n",
       " 'start',\n",
       " 'sex',\n",
       " 'perhaps',\n",
       " 'mean',\n",
       " 'came',\n",
       " 'others',\n",
       " 'moments',\n",
       " 'let',\n",
       " 'recommend',\n",
       " 'wonderful',\n",
       " 'episode',\n",
       " 'face',\n",
       " 'understand',\n",
       " 'small',\n",
       " 'school',\n",
       " 'terrible',\n",
       " 'playing',\n",
       " 'getting',\n",
       " 'written',\n",
       " 'doing',\n",
       " 'often',\n",
       " 'style',\n",
       " 'name',\n",
       " 'keep',\n",
       " 'perfect',\n",
       " 'early',\n",
       " 'human',\n",
       " 'definitely',\n",
       " 'gives',\n",
       " 'itself',\n",
       " 'lines',\n",
       " 'lost',\n",
       " 'live',\n",
       " 'person',\n",
       " 'become',\n",
       " 'dialogue',\n",
       " 'piece',\n",
       " 'finally',\n",
       " 'yes',\n",
       " 'head',\n",
       " 'case',\n",
       " 'felt',\n",
       " 'supposed',\n",
       " 'liked',\n",
       " 'couldnt',\n",
       " 'title',\n",
       " 'boy',\n",
       " 'white',\n",
       " 'absolutely',\n",
       " 'against',\n",
       " 'picture',\n",
       " 'sort',\n",
       " 'worse',\n",
       " 'cinema',\n",
       " 'went',\n",
       " 'shes',\n",
       " 'certainly',\n",
       " 'entire',\n",
       " 'waste',\n",
       " 'problem',\n",
       " 'oh',\n",
       " 'mr',\n",
       " 'evil',\n",
       " 'hope',\n",
       " 'entertaining',\n",
       " 'overall',\n",
       " 'called',\n",
       " 'based',\n",
       " 'fans',\n",
       " 'loved',\n",
       " 'several',\n",
       " 'mother',\n",
       " 'drama',\n",
       " 'id',\n",
       " 'killer',\n",
       " 'beginning',\n",
       " 'lives',\n",
       " 'direction',\n",
       " 'care',\n",
       " 'dark',\n",
       " 'becomes',\n",
       " 'already',\n",
       " 'guys',\n",
       " 'laugh',\n",
       " 'friend',\n",
       " 'example',\n",
       " 'under',\n",
       " 'despite',\n",
       " 'seemed',\n",
       " 'throughout',\n",
       " 'turn',\n",
       " 'unfortunately',\n",
       " 'wanted',\n",
       " 'children',\n",
       " 'final',\n",
       " 'history',\n",
       " 'fine',\n",
       " 'girls',\n",
       " 'amazing',\n",
       " 'sound',\n",
       " 'heart',\n",
       " 'guess',\n",
       " 'humor',\n",
       " 'totally',\n",
       " 'b',\n",
       " 'lead',\n",
       " 'writing',\n",
       " 'quality',\n",
       " 'wont',\n",
       " 'days',\n",
       " 'michael',\n",
       " 'wants',\n",
       " 'son',\n",
       " 'close',\n",
       " 'youll',\n",
       " 'art',\n",
       " 'works',\n",
       " 'behind',\n",
       " 'side',\n",
       " 'game',\n",
       " 'tries',\n",
       " 'ill',\n",
       " 'past',\n",
       " 'child',\n",
       " 'able',\n",
       " 'hand',\n",
       " 'turns',\n",
       " 'flick',\n",
       " 'act',\n",
       " 'theyre',\n",
       " 'enjoyed',\n",
       " 'town',\n",
       " 'genre',\n",
       " 'favorite',\n",
       " 'kill',\n",
       " 'starts',\n",
       " 'soon',\n",
       " 'eyes',\n",
       " 'car',\n",
       " 'sometimes',\n",
       " 'run',\n",
       " 'gave',\n",
       " 'actress',\n",
       " 'etc',\n",
       " 'late',\n",
       " 'ones',\n",
       " 'directed',\n",
       " 'horrible',\n",
       " 'viewer',\n",
       " 'brilliant',\n",
       " 'parts',\n",
       " 'hour',\n",
       " 'self',\n",
       " 'themselves',\n",
       " 'blood',\n",
       " 'stories',\n",
       " 'thinking',\n",
       " 'expect',\n",
       " 'stuff',\n",
       " 'city',\n",
       " 'obviously',\n",
       " 'decent',\n",
       " 'voice',\n",
       " 'highly',\n",
       " 'myself',\n",
       " 'fight',\n",
       " 'feeling',\n",
       " 'slow',\n",
       " 'except',\n",
       " 'hell',\n",
       " 'matter',\n",
       " 'kid',\n",
       " 'type',\n",
       " 'god',\n",
       " 'anyway',\n",
       " 'roles',\n",
       " 'age',\n",
       " 'says',\n",
       " 'killed',\n",
       " 'heard',\n",
       " 'moment',\n",
       " 'leave',\n",
       " 'took',\n",
       " 'writer',\n",
       " 'strong',\n",
       " 'cannot',\n",
       " 'police',\n",
       " 'violence',\n",
       " 'stop',\n",
       " 'hit',\n",
       " 'happens',\n",
       " 'known',\n",
       " 'particularly',\n",
       " 'involved',\n",
       " 'happened',\n",
       " 'extremely',\n",
       " 'chance',\n",
       " 'daughter',\n",
       " 'obvious',\n",
       " 'wouldnt',\n",
       " 'told',\n",
       " 'murder',\n",
       " 'living',\n",
       " 'coming',\n",
       " 'alone',\n",
       " 'experience',\n",
       " 'lack',\n",
       " 'james',\n",
       " 'including',\n",
       " 'attempt',\n",
       " 'please',\n",
       " 'happen',\n",
       " 'crap',\n",
       " 'brother',\n",
       " 'wonder',\n",
       " 'gore',\n",
       " 'complete',\n",
       " 'interest',\n",
       " 'cut',\n",
       " 'ago',\n",
       " 'none',\n",
       " 'score',\n",
       " 'group',\n",
       " 'simple',\n",
       " 'save',\n",
       " 'ok',\n",
       " 'looked',\n",
       " 'lets',\n",
       " 'song',\n",
       " 'number',\n",
       " 'career',\n",
       " 'seriously',\n",
       " 'possible',\n",
       " 'hero',\n",
       " 'annoying',\n",
       " 'sad',\n",
       " 'shown',\n",
       " 'exactly',\n",
       " 'running',\n",
       " 'musical',\n",
       " 'yourself',\n",
       " 'serious',\n",
       " 'scary',\n",
       " 'taken',\n",
       " 'whose',\n",
       " 'released',\n",
       " 'reality',\n",
       " 'david',\n",
       " 'hours',\n",
       " 'english',\n",
       " 'ends',\n",
       " 'cinematography',\n",
       " 'usually',\n",
       " 'opening',\n",
       " 'jokes',\n",
       " 'today',\n",
       " 'light',\n",
       " 'hilarious',\n",
       " 'cool',\n",
       " 'across',\n",
       " 'body',\n",
       " 'somewhat',\n",
       " 'usual',\n",
       " 'happy',\n",
       " 'view',\n",
       " 'ridiculous',\n",
       " 'started',\n",
       " 'relationship',\n",
       " 'level',\n",
       " 'change',\n",
       " 'opinion',\n",
       " 'middle',\n",
       " 'talking',\n",
       " 'taking',\n",
       " 'wish',\n",
       " 'order',\n",
       " 'husband',\n",
       " 'finds',\n",
       " 'shots',\n",
       " 'documentary',\n",
       " 'saying',\n",
       " 'episodes',\n",
       " 'power',\n",
       " 'huge',\n",
       " 'room',\n",
       " 'female',\n",
       " 'novel',\n",
       " 'mostly',\n",
       " 'robert',\n",
       " 'directors',\n",
       " 'talent',\n",
       " 'five',\n",
       " 'important',\n",
       " 'rating',\n",
       " 'modern',\n",
       " 'strange',\n",
       " 'major',\n",
       " 'word',\n",
       " 'turned',\n",
       " 'call',\n",
       " 'single',\n",
       " 'apparently',\n",
       " 'disappointed',\n",
       " 'events',\n",
       " 'songs',\n",
       " 'four',\n",
       " 'king',\n",
       " 'due',\n",
       " 'earth',\n",
       " 'basically',\n",
       " 'attention',\n",
       " 'knows',\n",
       " 'country',\n",
       " 'television',\n",
       " 'supporting',\n",
       " 'non',\n",
       " 'future',\n",
       " 'comic',\n",
       " 'clearly',\n",
       " 'knew',\n",
       " 'british',\n",
       " 'fast',\n",
       " 'thriller',\n",
       " 'd',\n",
       " 'class',\n",
       " 'easily',\n",
       " 'cheap',\n",
       " 'silly',\n",
       " 'problems',\n",
       " 'arent',\n",
       " 'words',\n",
       " 'miss',\n",
       " 'jack',\n",
       " 'tells',\n",
       " 'local',\n",
       " 'sequence',\n",
       " 'entertainment',\n",
       " 'bring',\n",
       " 'paul',\n",
       " 'beyond',\n",
       " 'whats',\n",
       " 'straight',\n",
       " 'upon',\n",
       " 'whether',\n",
       " 'romantic',\n",
       " 'sets',\n",
       " 'predictable',\n",
       " 'moving',\n",
       " 'similar',\n",
       " 'viewers',\n",
       " 'falls',\n",
       " 'mystery',\n",
       " 'rock',\n",
       " 'review',\n",
       " 'eye',\n",
       " 'oscar',\n",
       " 't',\n",
       " 'george',\n",
       " 'talk',\n",
       " 'enjoyable',\n",
       " 're',\n",
       " 'needs',\n",
       " 'appears',\n",
       " 'richard',\n",
       " 'giving',\n",
       " 'clich',\n",
       " 'within',\n",
       " 'ten',\n",
       " 'message',\n",
       " 'animation',\n",
       " 'near',\n",
       " 'theater',\n",
       " 'lady',\n",
       " 'above',\n",
       " 'sequel',\n",
       " 'theme',\n",
       " 'red',\n",
       " 'dull',\n",
       " 'stand',\n",
       " 'points',\n",
       " 'nearly',\n",
       " 'bunch',\n",
       " 'mention',\n",
       " 'herself',\n",
       " 'feels',\n",
       " 'add',\n",
       " 'lots',\n",
       " 'havent',\n",
       " 'team',\n",
       " 'release',\n",
       " 'ways',\n",
       " 'storyline',\n",
       " 'surprised',\n",
       " 'using',\n",
       " 'named',\n",
       " 'easy',\n",
       " 'fantastic',\n",
       " 'begins',\n",
       " 'die',\n",
       " 'working',\n",
       " 'actual',\n",
       " 'feature',\n",
       " 'effort',\n",
       " 'york',\n",
       " 'hate',\n",
       " 'french',\n",
       " 'tale',\n",
       " 'stay',\n",
       " 'minute',\n",
       " 'follow',\n",
       " 'e',\n",
       " 'clear',\n",
       " 'elements',\n",
       " 'among',\n",
       " 'comments',\n",
       " 'sister',\n",
       " 'typical',\n",
       " 'avoid',\n",
       " 'showing',\n",
       " 'editing',\n",
       " 'parents',\n",
       " 'tried',\n",
       " 'famous',\n",
       " 'sorry',\n",
       " 'fall',\n",
       " 'check',\n",
       " 'dialog',\n",
       " 'season',\n",
       " 'period',\n",
       " 'form',\n",
       " 'filmed',\n",
       " 'certain',\n",
       " 'buy',\n",
       " 'weak',\n",
       " 'tom',\n",
       " 'soundtrack',\n",
       " 'means',\n",
       " 'material',\n",
       " 'realistic',\n",
       " 'somehow',\n",
       " 'figure',\n",
       " 'general',\n",
       " 'crime',\n",
       " 'leads',\n",
       " 'doubt',\n",
       " 'space',\n",
       " 'gone',\n",
       " 'peter',\n",
       " 'viewing',\n",
       " 'kept',\n",
       " 'th',\n",
       " 'greatest',\n",
       " 'dance',\n",
       " 'lame',\n",
       " 'suspense',\n",
       " 'third',\n",
       " 'imagine',\n",
       " 'brought',\n",
       " 'atmosphere',\n",
       " 'hear',\n",
       " 'zombie',\n",
       " 'whos',\n",
       " 'whatever',\n",
       " 'sequences',\n",
       " 'particular',\n",
       " 'de',\n",
       " 'move',\n",
       " 'lee',\n",
       " 'indeed',\n",
       " 'reviews',\n",
       " 'rent',\n",
       " 'learn',\n",
       " 'eventually',\n",
       " 'average',\n",
       " 'wait',\n",
       " 'note',\n",
       " 'forget',\n",
       " 'writers',\n",
       " 'deal',\n",
       " 'surprise',\n",
       " 'japanese',\n",
       " 'okay',\n",
       " 'stage',\n",
       " 'sexual',\n",
       " 'poorly',\n",
       " 'premise',\n",
       " 'believable',\n",
       " 'sit',\n",
       " 'possibly',\n",
       " 'youve',\n",
       " 'decided',\n",
       " 'expected',\n",
       " 'subject',\n",
       " 'nature',\n",
       " 'dr',\n",
       " 'truth',\n",
       " 'street',\n",
       " 'free',\n",
       " 'became',\n",
       " 'difficult',\n",
       " 'screenplay',\n",
       " 'killing',\n",
       " 'romance',\n",
       " 'hot',\n",
       " 'nor',\n",
       " 'reading',\n",
       " 'question',\n",
       " 'needed',\n",
       " 'leaves',\n",
       " 'boys',\n",
       " 'baby',\n",
       " 'credits',\n",
       " 'meets',\n",
       " 'begin',\n",
       " 'unless',\n",
       " 'dog',\n",
       " 'superb',\n",
       " 'otherwise',\n",
       " 'imdb',\n",
       " 'write',\n",
       " 'shame',\n",
       " 'situation',\n",
       " 'meet',\n",
       " 'joe',\n",
       " 'dramatic',\n",
       " 'memorable',\n",
       " 'open',\n",
       " 'male',\n",
       " 'disney',\n",
       " 'earlier',\n",
       " 'badly',\n",
       " 'weird',\n",
       " 'forced',\n",
       " 'dream',\n",
       " 'acted',\n",
       " 'sci',\n",
       " 'laughs',\n",
       " 'emotional',\n",
       " 'society',\n",
       " 'fi',\n",
       " 'crazy',\n",
       " 'older',\n",
       " 'realize',\n",
       " 'beauty',\n",
       " 'deep',\n",
       " 'interested',\n",
       " 'forward',\n",
       " 'footage',\n",
       " 'comment',\n",
       " 'fantasy',\n",
       " 'sounds',\n",
       " 'america',\n",
       " 'whom',\n",
       " 'plus',\n",
       " 'directing',\n",
       " 'keeps',\n",
       " 'features',\n",
       " 'development',\n",
       " 'ask',\n",
       " 'mess',\n",
       " 'quickly',\n",
       " 'air',\n",
       " 'creepy',\n",
       " 'box',\n",
       " 'towards',\n",
       " 'perfectly',\n",
       " 'mark',\n",
       " 'worked',\n",
       " 'unique',\n",
       " 'monster',\n",
       " 'hands',\n",
       " 'cheesy',\n",
       " 'total',\n",
       " 'setting',\n",
       " 'plenty',\n",
       " 'fire',\n",
       " 'effect',\n",
       " 'result',\n",
       " 'brings',\n",
       " 'previous',\n",
       " 'personal',\n",
       " 'incredibly',\n",
       " 'rate',\n",
       " 'return',\n",
       " 'business',\n",
       " 'brothers',\n",
       " 'casting',\n",
       " 'apart',\n",
       " 'leading',\n",
       " 'joke',\n",
       " 'christmas',\n",
       " 'admit',\n",
       " 'powerful',\n",
       " 'background',\n",
       " 'appear',\n",
       " 'girlfriend',\n",
       " 'present',\n",
       " 'meant',\n",
       " 'telling',\n",
       " 'bill',\n",
       " 'hardly',\n",
       " 'battle',\n",
       " 'potential',\n",
       " 'create',\n",
       " 'break',\n",
       " 'masterpiece',\n",
       " 'secret',\n",
       " 'pay',\n",
       " 'political',\n",
       " 'gay',\n",
       " 'dumb',\n",
       " 'fighting',\n",
       " 'twist',\n",
       " 'fails',\n",
       " 'jane',\n",
       " 'era',\n",
       " 'cop',\n",
       " 'various',\n",
       " 'co',\n",
       " 'portrayed',\n",
       " 'inside',\n",
       " 'outside',\n",
       " 'western',\n",
       " 'reasons',\n",
       " 'nudity',\n",
       " 'ideas',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now convert the reviews in the train and test data sets to the word indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.map(lambda text, labels: (int_vectorize_layer(text), labels))\n",
    "\n",
    "test_set = test_set.map(lambda text, labels: (int_vectorize_layer(text), labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  14    4    1  333    5 4302   10   25    1   75  676   32 5066    5\n",
      "   24 2617  256   24  490 1343   33  569 1845  734  972    5  400  109\n",
      "   31  171  642    8 6262 3441  463   95   30   13    4    1 5389    5\n",
      " 1265   12   98   27  196 8200   40 6526   14    2 1313    1    8    4\n",
      "   93   26   13    4 5251    1    3    1   26   98   27  615    3  631\n",
      "    8    2  168 4086   26    1 3086    1 7556 6495 3911    3    1   16\n",
      " 5250    3 3380    9  199   27    4  739   18   26   13    4 1929  557\n",
      "    3    9    7    2  208   12    7   36  394 1001   37   24 7406   31\n",
      "    2   55    5  480 2329  986    7  108    1    8 1415  803   20 2764\n",
      "  687   30    5    2 4532    3 5526    7   65   18  689    5    2 1265\n",
      " 3127    3 3513    1    2  951    7   41    4  453 6313  579    2   61\n",
      " 7600   32    4 3913  244   71    4 7712   29   21   63 4302   31   30\n",
      " 2329   20    2   82  499    7   73 2419    6    2  940    2 8861    5\n",
      "    1    7  939 9604   37    2    1    1    6    2 3695    1    2  349\n",
      "  752  122  197    2    1    3  435    1    8   60    2 1150    7 2185\n",
      "   14    4 1944    4 2620  418   25   75  125  221 1299    1    7   33\n",
      " 4152 1087   18    2    1 5526    7   79   46    2    1 9609    5    2\n",
      " 1094    2 1031    1 3380    3 5250   23   30 1343   41   14 1333   14\n",
      "    2 1280   59   25 4414    3   91   46    7  816 2695 4327    7    4\n",
      " 2456   14    2 2900 2905 8574 1850 2329 5176   45  973 8655    6 3177\n",
      "  376   21 1217 6910 8783  304    4 9030   37    1 2835  127  298    4\n",
      " 8095   16 8402   32  734    1], shape=(300,), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for text, label in train_set.take(1):\n",
    "    print(text)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the words have bee nreplaced by the index numbers while the outcomes (1s and 0s) are the same. We are now ready to train the model. We will use an embedding layer in order to map these 10000 words to a 50 dimensional output. We will then feed this as input to an LSTM layer. Finally, we will use a single node in the output layer with a sigmoid activation function in order to pedict a 1 or a 0. Note that we have set the mask_zero parameter to True. This parameter simply tells the embedding layer to ignore the index 0. Why did we do this? Remember that we specified a word length of 300 in the vectorizer. Any review that is less than this will be padded with zeros. So by setting the mask_zero parameter to true, we are simply telling the layer to ignore these zeros since they do not provide any meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 [==============================] - 99s 2s/step - loss: 0.5960 - accuracy: 0.6708 - val_loss: 0.4324 - val_accuracy: 0.8150\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 102s 2s/step - loss: 0.3207 - accuracy: 0.8714 - val_loss: 0.3224 - val_accuracy: 0.8664\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 125s 3s/step - loss: 0.2286 - accuracy: 0.9172 - val_loss: 0.3460 - val_accuracy: 0.8636\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 105s 2s/step - loss: 0.2121 - accuracy: 0.9251 - val_loss: 0.3740 - val_accuracy: 0.8602\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 122s 3s/step - loss: 0.1926 - accuracy: 0.9296 - val_loss: 0.4004 - val_accuracy: 0.8504\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 105s 2s/step - loss: 0.1703 - accuracy: 0.9396 - val_loss: 0.3711 - val_accuracy: 0.8568\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 110s 2s/step - loss: 0.1609 - accuracy: 0.9437 - val_loss: 0.5277 - val_accuracy: 0.8253\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 115s 2s/step - loss: 0.1819 - accuracy: 0.9357 - val_loss: 0.4306 - val_accuracy: 0.8396\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 113s 2s/step - loss: 0.1366 - accuracy: 0.9529 - val_loss: 0.4281 - val_accuracy: 0.8516\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 109s 2s/step - loss: 0.1185 - accuracy: 0.9617 - val_loss: 0.4618 - val_accuracy: 0.8487\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    keras.layers.Embedding(vocabulary_size, 50, input_shape=[None], mask_zero=True),\n",
    "    tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "history = model.fit(train_set.batch(512), epochs=10, validation_data=test_set.batch(512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other option, as mentioned earlier, is to simply use a pre-trained embedding. This is actually easier than what we did because the prep-prepared embedding will do the work for us. Here is how we can load the pre-trained embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding is a 'Token based text embedding trained on English Google News 7B corpus'. There are other embeddings to choose from. This embedding maps the text to a 50-dimensional embedding vector. Let us try it out. First, we need to re-load the train adn test data sets because we had already vectorized the words for the previous model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"Oh yeah! Jenna Jameson did it again! Yeah Baby! This movie rocks. It was one of the 1st movies i saw of her. And i have to say i feel in love with her, she was great in this move.<br /><br />Her performance was outstanding and what i liked the most was the scenery and the wardrobe it was amazing you can tell that they put a lot into the movie the girls cloth were amazing.<br /><br />I hope this comment helps and u can buy the movie, the storyline is awesome is very unique and i'm sure u are going to like it. Jenna amazed us once more and no wonder the movie won so many awards. Her make-up and wardrobe is very very sexy and the girls on girls scene is amazing. specially the one where she looks like an angel. It's a must see and i hope u share my interests\", shape=(), dtype=string)\n",
      "tf.Tensor(\n",
      "[[ 0.42595312  0.41425827  0.05380295  0.65693736  0.02124309 -0.34377313\n",
      "   0.27501962 -0.29560134 -0.8899341   0.4253666  -0.03075071  0.15959139\n",
      "   0.05135492  0.41358554 -0.18948269 -0.29669785 -0.0527271   0.20711857\n",
      "   0.17791842 -0.88236904 -0.10125547 -0.2817679   0.530722    0.15064907\n",
      "  -0.47843954 -0.12719475 -1.1635325  -0.07020512  0.41874135 -0.14874929\n",
      "  -0.45107752  0.48977408  0.58515495 -0.1977401  -0.43619454  0.2391829\n",
      "   0.60983413 -0.26845714  0.17957966 -0.8037251  -0.01302063  0.18136978\n",
      "  -0.37247276  0.2534943  -0.25402808 -0.07823166 -0.5517427  -0.51032007\n",
      "  -0.15106294  0.21213137]], shape=(1, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_set = datasets[\"train\"]\n",
    "test_set = datasets[\"test\"]\n",
    "\n",
    "for text, label in train_set.take(1):\n",
    "    print(text)\n",
    "    print(hub_layer([text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we had to do in the above code is to feed the text to the pre-trained embedding model, and it does everything for you. So while previously we had to vectorize the text and then specify an embedding layer, we can now just use the pre-trained embedding layer. We now build the NN model and simply use the data sets as they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    hub_layer,\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 17s 341ms/step - loss: 0.5493 - accuracy: 0.8038 - val_loss: 0.4690 - val_accuracy: 0.8472\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 14s 290ms/step - loss: 0.2822 - accuracy: 0.9650 - val_loss: 0.3377 - val_accuracy: 0.8656\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 13s 258ms/step - loss: 0.1448 - accuracy: 0.9805 - val_loss: 0.3084 - val_accuracy: 0.8687\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 13s 260ms/step - loss: 0.0889 - accuracy: 0.9868 - val_loss: 0.3143 - val_accuracy: 0.8676\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 13s 258ms/step - loss: 0.0600 - accuracy: 0.9921 - val_loss: 0.3303 - val_accuracy: 0.8664\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 13s 258ms/step - loss: 0.0420 - accuracy: 0.9956 - val_loss: 0.3513 - val_accuracy: 0.8631\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 13s 258ms/step - loss: 0.0301 - accuracy: 0.9977 - val_loss: 0.3729 - val_accuracy: 0.8611\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 13s 260ms/step - loss: 0.0221 - accuracy: 0.9990 - val_loss: 0.3959 - val_accuracy: 0.8590\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 13s 262ms/step - loss: 0.0165 - accuracy: 0.9992 - val_loss: 0.4168 - val_accuracy: 0.8585\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 13s 262ms/step - loss: 0.0127 - accuracy: 0.9997 - val_loss: 0.4369 - val_accuracy: 0.8577\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "history = model.fit(train_set.shuffle(10000).batch(512), epochs=10, validation_data=test_set.batch(512))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
